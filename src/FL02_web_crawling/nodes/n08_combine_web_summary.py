# The MIT License (MIT)
# Copyright (c) 2025 Jozef Darida  (LinkedIn/Xing)
# For full license text, see the LICENSE file in the project root.

"""Node responsible for combining generated web content summary components into final files."""

import logging
import re
import sys
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Final, Literal, Optional, cast

from typing_extensions import TypeAlias

from sourcelens.core import BaseNode, SLSharedContext
from sourcelens.core.common_types import WebContentConceptsList, WebContentRelationshipsDict
from sourcelens.utils.helpers import sanitize_filename

CombineWebSummaryPreparedInputs: TypeAlias = bool
CombineWebSummaryExecutionResult: TypeAlias = None

WebContentConceptItem: TypeAlias = dict[str, Any]
WebChapterOrderList: TypeAlias = list[int]
WebChapterContentList: TypeAlias = list[str]
WebChapterTypeLiteral: TypeAlias = Literal[
    "standard", "inventory", "review", "transcript_orig", "transcript_final", "youtube_page"
]
WebChapterFileData: TypeAlias = dict[str, Any]
AllWebChaptersDataList: TypeAlias = list[WebChapterFileData]
ConfigDictTyped: TypeAlias = dict[str, Any]
LlmConfigDictTyped: TypeAlias = dict[str, Any]
OutputConfigDictTyped: TypeAlias = dict[str, Any]
SharedDataForWebCombine: TypeAlias = dict[str, Any]

module_logger_combine_web: logging.Logger = logging.getLogger(__name__)

WEB_INVENTORY_CHAPTER_TITLE: Final[str] = "Content Chunk Inventory"
WEB_INVENTORY_FILENAME_BASE: Final[str] = "content_inventory"
WEB_REVIEW_CHAPTER_TITLE: Final[str] = "Content Review"
WEB_REVIEW_FILENAME_BASE: Final[str] = "content_review"
WEB_TRANSCRIPT_ORIG_TITLE_PREFIX: Final[str] = "Original Transcript"
WEB_TRANSCRIPT_FINAL_TITLE_PREFIX: Final[str] = "Final Transcript"
WEB_TRANSCRIPT_FILENAME_PREFIX: Final[str] = "ts"
YOUTUBE_PAGE_FILENAME_PREFIX: Final[str] = "pg"
YOUTUBE_PAGE_CONTENT_SUBDIR: Final[str] = "page_content"


WEB_INDEX_FILENAME: Final[str] = "web_summary_index.md"
WEB_FOOTER_SEPARATOR: Final[str] = "\n\n---\n\n*Generated by"
WEB_PREV_LINK_REGEX: Final[re.Pattern[str]] = re.compile(
    r"^\s*(?:> ?)?Previously, we looked at\s+\[.*?\]\(.*?\)\.?\s*$",
    re.IGNORECASE | re.MULTILINE,
)
WEB_NEXT_LINK_REGEX: Final[re.Pattern[str]] = re.compile(
    r"^\s*(?:> ?)?Next, we will examine\s+\[.*?\]\(.*?\)\.?\s*$",
    re.IGNORECASE | re.MULTILINE,
)
WEB_H1_HEADING_REGEX: Final[re.Pattern[str]] = re.compile(r"^\s*#\s+.*$", re.MULTILINE)
DEFAULT_WEB_COLLECTION_NAME_COMBINE: Final[str] = "web_content_summary"
DEFAULT_OUTPUT_DIR_WEB_COMBINE: Final[str] = "output"
FOOTER_PROVIDER_DEFAULT_WEB: Final[str] = "N/A"
FOOTER_MODEL_DEFAULT_WEB: Final[str] = "N/A"
FOOTER_TARGET_LANG_DEFAULT_WEB: Final[str] = "english"
FILENAME_WEB_CHAPTER_PREFIX_WIDTH: Final[int] = 2
LOG_SNIPPET_LEN_WEB_COMBINE: Final[int] = 200
MAX_INT_FOR_SORTING_WEB: Final[int] = sys.maxsize
MINIMAL_INDEX_CONTENT_MARKER: Final[str] = "No chapters generated for this summary."
TRANSCRIPTS_SUBDIR_NAME_COMBINE: Final[str] = "transcripts"


@dataclass(frozen=True)
class WebFooterInfo:
    """Encapsulate information for the web summary file footer."""

    provider_name: str
    model_name: str
    is_local: bool
    target_language: str

    def format_footer(self) -> str:
        """Generate the formatted footer string.

        Returns:
            str: The formatted footer string.
        """
        location: str = "(local)" if self.is_local else "(cloud)"
        repo_link: str = "https://github.com/openXFlow/sourceLensAI"
        part1: str = f"{WEB_FOOTER_SEPARATOR} [SourceLens AI]({repo_link}) "
        part2: str = f"using LLM: `{self.provider_name}` {location} "
        part3: str = f"- model: `{self.model_name}` | Target Language: `{self.target_language}`*"
        return part1 + part2 + part3


@dataclass(frozen=True)
class WebIndexContext:
    """Encapsulate context for the web summary index.md file."""

    collection_name: str
    web_relationships_data: WebContentRelationshipsDict
    footer_info: WebFooterInfo
    is_youtube_content: bool
    original_source_url: Optional[str] = None
    chapter_files_data: AllWebChaptersDataList = field(default_factory=list)
    youtube_page_filename: Optional[str] = None


class CombineWebSummary(BaseNode[CombineWebSummaryPreparedInputs, CombineWebSummaryExecutionResult]):
    """Combine generated web content summary components into final Markdown files."""

    def _get_special_chapter_base_name(self, chapter_type: WebChapterTypeLiteral) -> str:
        """Return the base filename for a special web chapter type.

        Args:
            chapter_type: The type of the special chapter.

        Returns:
            The base filename string for that chapter type.
        """
        if chapter_type == "inventory":
            return WEB_INVENTORY_FILENAME_BASE
        if chapter_type == "review":
            return WEB_REVIEW_FILENAME_BASE
        self._log_warning("Unknown special web chapter type '%s' for filename base.", chapter_type)  # pragma: no cover
        return "special_web_chapter"  # pragma: no cover

    def _add_prev_link(self, chapter_data: WebChapterFileData, prev_link_text: Optional[str]) -> None:
        """Ensure the 'Previously...' navigation link exists once before H1 heading.

        Args:
            chapter_data: Dictionary of chapter data.
            prev_link_text: The Markdown link text for the previous chapter.
        """
        if not prev_link_text or not prev_link_text.strip():
            return
        content: str = str(chapter_data.get("content", "") or "")
        chapter_num_str = str(chapter_data.get("num", "Unknown"))
        content, num_removed = WEB_PREV_LINK_REGEX.subn("", content)
        if num_removed > 0:
            self._log_info("Removed %d 'Previously...' link(s) from web ch %s.", num_removed, chapter_num_str)
        content = re.sub(r"^\s*\n", "", content, flags=re.MULTILINE)
        content = re.sub(r"\n{3,}", "\n\n", content).strip()
        h1_match = WEB_H1_HEADING_REGEX.search(content)
        if h1_match:
            start_index = h1_match.start()
            prefix_newline = "\n" if start_index > 0 and not content[:start_index].isspace() else ""
            new_content_parts = [
                content[:start_index].rstrip(),
                prefix_newline,
                f"\n\n{prev_link_text}\n\n",
                content[start_index:],
            ]
            content = "".join(filter(None, new_content_parts))
        else:  # pragma: no cover
            content = f"{prev_link_text}\n\n{content}"
            self._log_warning("No H1 in web ch %s. Added 'Previously...' link at start.", chapter_num_str)
        chapter_data["content"] = content.strip()

    def _add_next_link(self, chapter_data: WebChapterFileData, next_link_text: Optional[str]) -> None:
        """Ensure the 'Next...' navigation link exists once at the end of chapter content.

        Args:
            chapter_data: Dictionary of chapter data.
            next_link_text: The Markdown link text for the next chapter.
        """
        if not next_link_text or not next_link_text.strip():
            return
        content: str = str(chapter_data.get("content", "") or "")
        chapter_num_str = str(chapter_data.get("num", "Unknown"))
        content, num_removed = WEB_NEXT_LINK_REGEX.subn("", content)
        if num_removed > 0:
            self._log_info("Removed %d 'Next...' link(s) from web ch %s.", num_removed, chapter_num_str)
        content = re.sub(r"\n{3,}", "\n\n", content).strip()
        content += f"\n\n{next_link_text}" if content else next_link_text
        chapter_data["content"] = content.strip()

    def _add_navigation_links(self, all_chapters_data: AllWebChaptersDataList, index_filename: str) -> None:
        """Add or update 'Previously...' and 'Next...' links in all web chapters.

        Args:
            all_chapters_data: List of all chapter data dictionaries.
            index_filename: The filename of the main index/overview page.
        """
        if not all_chapters_data:  # pragma: no cover
            self._log_warning("No web chapters found to add navigation links.")
            return

        navigable_chapters = [
            ch
            for ch in all_chapters_data
            if ch.get("chapter_type") not in ["transcript_orig", "transcript_final", "youtube_page"]
        ]
        num_navigable_chapters = len(navigable_chapters)
        if num_navigable_chapters == 0:
            self._log_info("No LLM-generated chapters found. Skipping navigation links between them.")
            return

        for i, chapter_data in enumerate(navigable_chapters):
            prev_link_text: Optional[str] = None
            if i == 0:
                prev_link_text = f"> Previously, we looked at the [Content Overview]({index_filename})."
            elif i > 0:
                prev_ch = navigable_chapters[i - 1]
                p_name = str(prev_ch.get("name", "")).strip()
                p_file_raw = str(prev_ch.get("filename", "")).strip()
                p_file = Path(p_file_raw).name if YOUTUBE_PAGE_CONTENT_SUBDIR in p_file_raw else p_file_raw
                if p_name and p_file:
                    prev_link_text = f"> Previously, we looked at [{p_name}]({p_file})."
                else:  # pragma: no cover
                    log_msg = f"Could not create 'previous' link for web ch {chapter_data.get('num', i + 1)}."
                    self._log_warning(log_msg)
            self._add_prev_link(chapter_data, prev_link_text)

            next_link_text: Optional[str] = None
            if i < num_navigable_chapters - 1:
                next_ch = navigable_chapters[i + 1]
                n_name = str(next_ch.get("name", "")).strip()
                n_file_raw = str(next_ch.get("filename", "")).strip()
                n_file = Path(n_file_raw).name if YOUTUBE_PAGE_CONTENT_SUBDIR in n_file_raw else n_file_raw
                if n_name and n_file:
                    next_link_text = f"> Next, we will examine [{n_name}]({n_file})."
                else:  # pragma: no cover
                    log_msg = f"Could not create 'next' link for web ch {chapter_data.get('num', i + 1)}."
                    self._log_warning(log_msg)
            self._add_next_link(chapter_data, next_link_text)

    def _add_footers(self, all_chapters_data: AllWebChaptersDataList, footer_text: str) -> None:
        """Append the standard footer to all web chapter contents.

        Args:
            all_chapters_data: List of all chapter data dictionaries.
            footer_text: The footer text to append.
        """
        clean_footer_text: str = footer_text.strip()
        if not clean_footer_text:  # pragma: no cover
            self._log_warning("Footer text is empty, not adding footers to web chapters.")
            return
        for chapter_data in all_chapters_data:
            if chapter_data.get("chapter_type") in ["transcript_orig", "transcript_final", "youtube_page"]:
                continue
            current_content: str = str(chapter_data.get("content", "") or "")
            if WEB_FOOTER_SEPARATOR in current_content:  # pragma: no cover
                current_content = current_content.split(WEB_FOOTER_SEPARATOR, 1)[0]
            current_content = current_content.rstrip()
            chapter_data["content"] = f"{current_content}\n{footer_text}" if current_content else footer_text.lstrip()

    def _prepare_web_index_content(self, context: WebIndexContext) -> str:
        """Prepare the full Markdown content for the web summary index.md file.

        Args:
            context: Context object with all necessary data.

        Returns:
            The complete Markdown content for the index file.
        """
        self._log_info("Preparing web summary index.md content for '%s'...", context.collection_name)
        summary_raw: Any = context.web_relationships_data.get(
            "overall_summary", f"Summary of content from {context.collection_name}."
        )
        summary: str = str(summary_raw or f"Summary from {context.collection_name}.")
        content_parts: list[str] = [f"# Web Content Summary: {context.collection_name}\n\n{summary}\n"]
        if context.original_source_url:
            content_parts.append(
                f"**Original Source:** [{context.original_source_url}]({context.original_source_url})\n"
            )

        if context.is_youtube_content and context.youtube_page_filename:
            content_parts.append(
                f"## Video Overview\n\n- [{context.collection_name} (Main Page)]({context.youtube_page_filename})\n"
            )

        content_parts.append("## Content Overview (Generated Chapters)\n")
        chapters_for_index = [
            ch
            for ch in context.chapter_files_data
            if ch.get("chapter_type") not in ["transcript_orig", "transcript_final", "youtube_page"]
        ]
        sorted_chapters_for_index = sorted(chapters_for_index, key=lambda x: x.get("num", MAX_INT_FOR_SORTING_WEB))

        chapter_links: list[str] = []
        for ch_data in sorted_chapters_for_index:
            num_val = ch_data.get("num")
            name_val = ch_data.get("name")
            fname_val_raw = ch_data.get("filename")
            if (
                isinstance(num_val, int)
                and num_val > 0
                and isinstance(name_val, str)
                and name_val.strip()
                and isinstance(fname_val_raw, str)
                and fname_val_raw.strip()
            ):
                fname_val = Path(fname_val_raw).name if YOUTUBE_PAGE_CONTENT_SUBDIR in fname_val_raw else fname_val_raw
                chapter_links.append(f"{num_val}. [{name_val.strip()}]({fname_val.strip()})")

        chapter_list_str = "\n".join(chapter_links) if chapter_links else MINIMAL_INDEX_CONTENT_MARKER
        content_parts.append(chapter_list_str)

        transcript_links: list[str] = []
        for ch_data in context.chapter_files_data:
            if ch_data.get("chapter_type") in ["transcript_orig", "transcript_final"]:
                name_val = ch_data.get("name")
                fname_val = ch_data.get("filename")
                if isinstance(name_val, str) and name_val.strip() and isinstance(fname_val, str) and fname_val.strip():
                    transcript_path_for_index = f"{TRANSCRIPTS_SUBDIR_NAME_COMBINE}/{fname_val.strip()}"
                    transcript_links.append(f"- [{name_val.strip()}]({transcript_path_for_index})")

        if transcript_links:
            content_parts.append("\n\n## Available Transcripts\n")
            content_parts.append("\n".join(transcript_links))
        return "\n".join(content_parts)

    def _prepare_standard_web_chapters_data(
        self,
        concepts: WebContentConceptsList,
        chapter_order: WebChapterOrderList,
        chapters_content: WebChapterContentList,
    ) -> AllWebChaptersDataList:
        """Prepare initial data structure for standard web chapter files.

        Args:
            concepts: List of identified web concepts.
            chapter_order: Ordered list of concept indices.
            chapters_content: List of generated Markdown for each chapter.

        Returns:
            A list of dictionaries, each representing a standard chapter.
        """
        std_chapters: AllWebChaptersDataList = []
        num_concepts = len(concepts)
        effective_count = min(len(chapter_order), len(chapters_content))

        if len(chapter_order) != len(chapters_content):  # pragma: no cover
            self._log_warning(
                "Web chapter order count (%d) != content count (%d). Processing %d.",
                len(chapter_order),
                len(chapters_content),
                effective_count,
            )
        for i in range(effective_count):
            concept_idx = chapter_order[i]
            if not (0 <= concept_idx < num_concepts):  # pragma: no cover
                self._log_warning("Invalid concept_idx %d in web chapter order. Skipping.", concept_idx)
                continue
            concept_item: WebContentConceptItem = concepts[concept_idx]
            name_raw: Any = concept_item.get("name", f"Topic {i + 1}")
            name: str = str(name_raw).strip() if isinstance(name_raw, str) and name_raw.strip() else f"Topic {i + 1}"
            std_chapters.append(
                {
                    "content": str(chapters_content[i] or ""),
                    "name": name,
                    "concept_index": concept_idx,
                    "chapter_type": "standard",
                    "filename": "",
                    "num": -1,
                }
            )
        self._log_info("Prepared data for %d standard web chapters.", len(std_chapters))
        return std_chapters

    def _prepare_special_web_chapter_data(
        self,
        content_key: str,
        title: str,
        chapter_type: WebChapterTypeLiteral,
        shared_context: SLSharedContext,
        filename_override: Optional[str] = None,
    ) -> Optional[WebChapterFileData]:
        """Prepare data structure for a special web chapter.

        Args:
            content_key: Key in shared_context holding the chapter's content or path.
            title: The title of this special chapter.
            chapter_type: The type of the special chapter.
            shared_context: The shared context dictionary.
            filename_override: If provided, use this as the filename (can include path).

        Returns:
            A dictionary with chapter data if content is found, else None.
        """
        content_val_any: Any = shared_context.get(content_key)
        actual_content_str: Optional[str] = None
        resolved_filename: Optional[str] = filename_override

        if chapter_type in ["transcript_orig", "transcript_final"]:
            if isinstance(content_val_any, str) and Path(content_val_any).is_file():
                try:
                    actual_content_str = Path(content_val_any).read_text(encoding="utf-8")
                    if not resolved_filename:
                        resolved_filename = Path(content_val_any).name
                except OSError as e:
                    self._log_error("Could not read file %s: %s", content_val_any, e)
                    return None  # pragma: no cover
            elif not content_val_any:
                self._log_info("Path for '%s' not found (key: '%s').", chapter_type, content_key)
                return None
            else:  # pragma: no cover
                self._log_warning("Invalid path for '%s': %s", chapter_type, content_val_any)
                return None
        elif isinstance(content_val_any, str) and content_val_any.strip():
            actual_content_str = content_val_any
        else:
            self._log_info("No content for '%s' (key: '%s').", chapter_type, content_key)
            return None

        if not actual_content_str or not actual_content_str.strip():  # pragma: no cover
            self._log_info("Content for '%s' is empty. Skipping.", chapter_type)
            return None

        self._log_info("'%s' data prepared. Title: %s", chapter_type.capitalize(), title)
        return {
            "filename": resolved_filename or "",
            "content": actual_content_str,
            "name": title,
            "num": -1,
            "concept_index": -1,
            "chapter_type": chapter_type,
        }

    def _add_llm_generated_chapters_to_list(
        self,
        all_chapters_list: AllWebChaptersDataList,
        shared_context: SLSharedContext,
        data: SharedDataForWebCombine,
    ) -> int:
        """Add standard, inventory, and review chapters to the list.

        Args:
            all_chapters_list: The list to append chapters to.
            shared_context: The main shared context.
            data: Data retrieved for this node.

        Returns:
            The updated current_chapter_num after adding these chapters.
        """
        current_chapter_num: int = 1
        flow_out_opts: OutputConfigDictTyped = cast(dict, shared_context.get("current_mode_output_options", {}))
        concepts: WebContentConceptsList = cast(list, data.get("text_concepts", []))
        order: WebChapterOrderList = cast(list, data.get("text_chapter_order", []))
        content_list: WebChapterContentList = cast(list, data.get("text_chapters", []))

        if concepts and order and content_list:
            all_chapters_list.extend(self._prepare_standard_web_chapters_data(concepts, order, content_list))

        for i, chap_data in enumerate(ch for ch in all_chapters_list if ch["chapter_type"] == "standard"):
            chap_data["num"] = i + 1
        current_chapter_num = len([ch for ch in all_chapters_list if ch["chapter_type"] == "standard"]) + 1

        if flow_out_opts.get("include_content_inventory", False):
            inv_ch = self._prepare_special_web_chapter_data(
                "content_inventory_md", WEB_INVENTORY_CHAPTER_TITLE, "inventory", shared_context
            )
            if inv_ch:
                inv_ch["num"] = current_chapter_num
                all_chapters_list.append(inv_ch)
                current_chapter_num += 1
        if flow_out_opts.get("include_content_review", False):
            rev_ch = self._prepare_special_web_chapter_data(
                "web_content_review_md", WEB_REVIEW_CHAPTER_TITLE, "review", shared_context
            )
            if rev_ch:
                rev_ch["num"] = current_chapter_num
                all_chapters_list.append(rev_ch)
        return current_chapter_num

    def _add_youtube_specific_files_to_list(
        self,
        all_chapters_list: AllWebChaptersDataList,
        shared_context: SLSharedContext,
        data: SharedDataForWebCombine,
        target_lang: str,
    ) -> Optional[str]:
        """Add YouTube page summary and transcript files to the list.

        The 'youtube_page' filename will now include the 'page_content/' subdirectory.

        Args:
            all_chapters_list: The list to append chapters to.
            shared_context: The main shared context.
            data: Data retrieved for this node.
            target_lang: The target language of the project.

        Returns:
            The filename (including subdirectory) of the generated YouTube page summary
            (e.g., "page_content/pg_...md") if created, else None.
        """
        youtube_page_filename_with_subdir: Optional[str] = None
        if shared_context.get("current_youtube_video_id"):
            video_title = str(data.get("current_youtube_video_title", "YouTube Video"))
            sanitized_title = str(shared_context.get("current_youtube_sanitized_title", sanitize_filename(video_title)))
            video_description = str(data.get("current_youtube_description", "No description available."))
            upload_date = str(data.get("current_youtube_upload_date", "N/A"))
            view_count_any: Any = data.get("current_youtube_view_count")
            view_count_str = f"{int(view_count_any):,}" if isinstance(view_count_any, int) else "N/A"
            uploader = str(data.get("current_youtube_uploader", "N/A"))
            original_source_url = str(data.get("original_source_url", "#"))

            pg_content_parts = [f"# {video_title}\n"]
            pg_content_parts.append(f"**Source:** [{original_source_url}]({original_source_url})")
            pg_content_parts.append(
                f"**Uploader:** {uploader}  \n**Uploaded:** {upload_date}  \n**Views:** {view_count_str}\n"
            )
            pg_content_parts.append(f"## Description\n\n{video_description}\n")

            orig_path = cast(Optional[str], data.get("current_youtube_standalone_transcript_path"))
            final_path = cast(Optional[str], data.get("current_youtube_final_transcript_path"))
            if orig_path or final_path:
                pg_content_parts.append("## Transcripts\n")
                if orig_path:
                    orig_lang_yt = str(data.get("current_youtube_original_lang", "original"))
                    orig_fname = Path(orig_path).name
                    pg_content_parts.append(
                        f"- [Original Transcript ({orig_lang_yt})]](../{TRANSCRIPTS_SUBDIR_NAME_COMBINE}/{orig_fname})"
                    )
                if final_path and final_path != orig_path:
                    final_lang_yt = str(data.get("current_youtube_final_transcript_lang", target_lang))
                    final_fname = Path(final_path).name
                    pg_content_parts.append(
                        f"- [Translated Transcript ({final_lang_yt})]("
                        f"../{TRANSCRIPTS_SUBDIR_NAME_COMBINE}/{final_fname})"
                    )

            youtube_page_filename_with_subdir = str(
                Path(YOUTUBE_PAGE_CONTENT_SUBDIR) / f"{YOUTUBE_PAGE_FILENAME_PREFIX}_{sanitized_title}.md"
            )
            all_chapters_list.append(
                {
                    "filename": youtube_page_filename_with_subdir,
                    "content": "\n".join(pg_content_parts),
                    "name": video_title,
                    "num": 0,
                    "chapter_type": "youtube_page",
                }
            )

            yt_orig_lang: str = str(data.get("current_youtube_original_lang", "src"))
            orig_title_ts = f"{WEB_TRANSCRIPT_ORIG_TITLE_PREFIX} ({yt_orig_lang})"
            orig_ts_chapter = self._prepare_special_web_chapter_data(
                "current_youtube_standalone_transcript_path",
                orig_title_ts,
                "transcript_orig",
                shared_context,
                filename_override=f"{WEB_TRANSCRIPT_FILENAME_PREFIX}_{sanitized_title}_{yt_orig_lang}_orig.md",
            )
            if orig_ts_chapter:
                orig_ts_chapter["num"] = MAX_INT_FOR_SORTING_WEB - 2
                all_chapters_list.append(orig_ts_chapter)

            yt_final_lang: str = str(data.get("current_youtube_final_transcript_lang", target_lang))
            final_title_ts = f"{WEB_TRANSCRIPT_FINAL_TITLE_PREFIX} ({yt_final_lang})"
            final_ts_chapter = self._prepare_special_web_chapter_data(
                "current_youtube_final_transcript_path",
                final_title_ts,
                "transcript_final",
                shared_context,
                filename_override=f"{WEB_TRANSCRIPT_FILENAME_PREFIX}_{sanitized_title}_{yt_final_lang}_final.md",
            )
            if final_ts_chapter:
                final_ts_chapter["num"] = MAX_INT_FOR_SORTING_WEB - 1
                all_chapters_list.append(final_ts_chapter)
        return youtube_page_filename_with_subdir

    def _finalize_chapter_numbering_and_filenames(self, all_chapters_list: AllWebChaptersDataList) -> None:
        """Sort chapters, re-number main content, and generate filenames.

        Filenames for 'youtube_page' are expected to already include their subdirectory.
        Filenames for 'transcript_orig' and 'transcript_final' are just the name part.

        Args:
            all_chapters_list: The list of all chapter data to finalize.
        """
        all_chapters_list.sort(key=lambda x: x.get("num", MAX_INT_FOR_SORTING_WEB))
        main_content_chapter_counter = 0
        temp_final_list: AllWebChaptersDataList = []

        for chapter_info in all_chapters_list:
            ch_type = cast(WebChapterTypeLiteral, chapter_info.get("chapter_type", "standard"))

            if ch_type == "youtube_page":
                temp_final_list.append(chapter_info)
            elif ch_type in ["transcript_orig", "transcript_final"]:
                temp_final_list.append(chapter_info)
            else:
                main_content_chapter_counter += 1
                chapter_info["num"] = main_content_chapter_counter
                name_raw = str(chapter_info.get("name", f"web-chapter-{chapter_info['num']}"))
                base_name_parts: list[str] = []
                if ch_type in ["inventory", "review"]:
                    base_name_parts.append(self._get_special_chapter_base_name(ch_type))
                else:
                    base_name_parts.append(sanitize_filename(name_raw) or f"chapter-{chapter_info['num']}")
                base_name = "-".join(bn_part for bn_part in base_name_parts if bn_part)
                num_val = cast(int, chapter_info["num"])
                chapter_info["filename"] = f"{num_val:0{FILENAME_WEB_CHAPTER_PREFIX_WIDTH}d}_{base_name}.md"
                temp_final_list.append(chapter_info)
        all_chapters_list[:] = temp_final_list

    def _assemble_all_web_chapters(
        self, shared_context: SLSharedContext, data: SharedDataForWebCombine, *, is_yt_content: bool
    ) -> tuple[AllWebChaptersDataList, Optional[str]]:
        """Assemble all chapter data, including special YouTube handling.

        Args:
            shared_context: The main shared context.
            data: Data retrieved specifically for this node.
            is_yt_content: True if processing YouTube content.

        Returns:
            A tuple: (list_of_all_chapter_data, youtube_page_filename_or_None).
            The `youtube_page_filename` will include the `page_content/` subdirectory.
        """
        all_chapters_list: AllWebChaptersDataList = []
        youtube_page_filename_with_subdir: Optional[str] = None
        target_lang: str = str(shared_context.get("language", "en"))

        config: dict[str, Any] = cast(dict, shared_context.get("config", {}))
        crawler_opts: dict[str, Any] = config.get("FL02_web_crawling", {}).get("crawler_options", {})
        processing_mode: str = str(crawler_opts.get("processing_mode", "minimalistic"))

        if processing_mode == "llm_extended":
            self._add_llm_generated_chapters_to_list(all_chapters_list, shared_context, data)

        if is_yt_content:
            youtube_page_filename_with_subdir = self._add_youtube_specific_files_to_list(
                all_chapters_list, shared_context, data, target_lang
            )
            if not all_chapters_list and not youtube_page_filename_with_subdir:  # pragma: no cover
                self._log_warning("No YouTube specific files (page or transcripts) were assembled.")

        self._finalize_chapter_numbering_and_filenames(all_chapters_list)
        self._log_info("Assembled and finalized %d total web files/chapters.", len(all_chapters_list))
        return all_chapters_list, youtube_page_filename_with_subdir

    def _retrieve_shared_data_for_web_combine(self, shared_context: SLSharedContext) -> SharedDataForWebCombine:
        """Retrieve all necessary data from shared context for web summary combination.

        Args:
            shared_context: The main shared context.

        Returns:
            A dictionary of required data.
        """
        self._logger.debug("--- Retrieving data from shared_context for CombineWebSummary ---")
        original_source: Optional[Any] = (
            shared_context.get("crawl_url")
            or shared_context.get("crawl_sitemap")
            or shared_context.get("crawl_file")
            or shared_context.get("current_youtube_url")
        )
        data: SharedDataForWebCombine = {
            "project_name": str(self._get_required_shared(shared_context, "project_name")),
            "output_base_dir": str(self._get_required_shared(shared_context, "output_dir")),
            "text_concepts": shared_context.get("text_concepts", []),
            "text_relationships": shared_context.get("text_relationships", {}),
            "text_chapter_order": shared_context.get("text_chapter_order", []),
            "text_chapters": shared_context.get("text_chapters", []),
            "content_inventory_md": shared_context.get("content_inventory_md"),
            "web_content_review_md": shared_context.get("web_content_review_md"),
            "llm_config": self._get_required_shared(shared_context, "llm_config"),
            "config": self._get_required_shared(shared_context, "config"),
            "original_source_url": str(original_source) if original_source is not None else None,
            "current_youtube_video_id": shared_context.get("current_youtube_video_id"),
            "current_youtube_video_title": shared_context.get("current_youtube_video_title"),
            "current_youtube_sanitized_title": shared_context.get("current_youtube_sanitized_title"),
            "current_youtube_description": shared_context.get("current_youtube_description"),
            "current_youtube_upload_date": shared_context.get("current_youtube_upload_date"),
            "current_youtube_view_count": shared_context.get("current_youtube_view_count"),
            "current_youtube_uploader": shared_context.get("current_youtube_uploader"),
            "current_youtube_standalone_transcript_path": shared_context.get(
                "current_youtube_standalone_transcript_path"
            ),
            "current_youtube_final_transcript_path": shared_context.get("current_youtube_final_transcript_path"),
            "current_youtube_original_lang": shared_context.get("current_youtube_original_lang"),
            "current_youtube_final_transcript_lang": shared_context.get("current_youtube_final_transcript_lang"),
        }
        self._logger.debug("--- Finished retrieving data from shared_context for web summary ---")
        return data

    def _initialize_output_context(
        self, shared_context: SLSharedContext, retrieved_data: SharedDataForWebCombine
    ) -> tuple[WebFooterInfo, Path]:
        """Initialize footer info and determine the output path.

        Args:
            shared_context: The main shared context.
            retrieved_data: Data specific to this node.

        Returns:
            Footer information and the resolved output path.
        """
        llm_cfg: LlmConfigDictTyped = cast(dict, retrieved_data.get("llm_config", {}))
        target_lang: str = str(shared_context.get("language", FOOTER_TARGET_LANG_DEFAULT_WEB))

        footer_obj = WebFooterInfo(
            provider_name=str(llm_cfg.get("provider", FOOTER_PROVIDER_DEFAULT_WEB)),
            model_name=str(llm_cfg.get("model", FOOTER_MODEL_DEFAULT_WEB)),
            is_local=bool(llm_cfg.get("is_local_llm", False)),
            target_language=target_lang,
        )
        project_name = str(retrieved_data.get("project_name", DEFAULT_WEB_COLLECTION_NAME_COMBINE))
        output_base_dir = str(retrieved_data.get("output_base_dir", DEFAULT_OUTPUT_DIR_WEB_COMBINE))
        output_path = Path(output_base_dir) / project_name
        return footer_obj, output_path.resolve()

    def _write_all_files(
        self, output_path: Path, index_content: str, all_chapters: AllWebChaptersDataList, footer_str: str
    ) -> bool:
        """Write index and chapter files to disk.

        Args:
            output_path: The base output directory for this summary.
            index_content: The Markdown content for the index file.
            all_chapters: List of all chapter data.
            footer_str: The footer string to append if not present.

        Returns:
            True if all writes were successful, False otherwise.
        """
        try:
            output_path.mkdir(parents=True, exist_ok=True)
            self._log_info("Ensured web summary output directory exists: %s", output_path)
        except OSError as e:
            self._log_error("CRITICAL: Failed to create dir %s: %s", output_path, e)
            return False  # pragma: no cover

        index_fpath = output_path / WEB_INDEX_FILENAME
        final_index_content = index_content.rstrip()
        if WEB_FOOTER_SEPARATOR not in final_index_content:  # pragma: no cover
            final_index_content += f"\n{footer_str}"

        try:
            index_fpath.write_text(final_index_content, encoding="utf-8")
            self._log_info("Wrote web summary index file: %s", index_fpath)
        except OSError as e:
            self._log_error("CRITICAL: Failed to write index %s: %s", index_fpath, e)
            return False  # pragma: no cover

        all_ok = True
        for ch_info in all_chapters:
            filename_with_path_str = str(ch_info.get("filename", ""))
            content = str(ch_info.get("content", ""))
            ch_type = cast(WebChapterTypeLiteral, ch_info.get("chapter_type", "standard"))

            if not filename_with_path_str or not content:  # pragma: no cover
                self._log_warning("Skipping write for '%s': missing filename/content.", ch_info.get("name"))
                all_ok = False
                continue

            save_path: Path
            if ch_type in ["transcript_orig", "transcript_final"]:
                transcript_sub_dir = output_path / TRANSCRIPTS_SUBDIR_NAME_COMBINE
                # filename_with_path_str is just the name for transcripts
                save_path = transcript_sub_dir / Path(filename_with_path_str).name
            else:
                # For 'youtube_page', filename_with_path_str is "page_content/pg_...md"
                # For others, it's like "01_chapter.md"
                save_path = output_path / filename_with_path_str

            try:
                save_path.parent.mkdir(parents=True, exist_ok=True)
                save_path.write_text(content, encoding="utf-8")
                self._log_info("Wrote web chapter/file: %s", save_path)
            except OSError as e:  # pragma: no cover
                self._log_error("Failed to write %s: %s", save_path, e)
                all_ok = False
        return all_ok

    def pre_execution(self, shared_context: SLSharedContext) -> CombineWebSummaryPreparedInputs:
        """Prepare all web summary content and write output files.

        Args:
            shared_context: The shared context dictionary.

        Returns:
            True if successful, False otherwise.
        """
        self._log_info(">>> CombineWebSummary.pre_execution: Assembling and writing web summary files. <<<")
        output_path_for_summary: Optional[Path] = None
        write_success: bool = False
        try:
            retrieved_data = self._retrieve_shared_data_for_web_combine(shared_context)
            footer_obj, output_path_for_summary = self._initialize_output_context(shared_context, retrieved_data)
            self._log_info("Final output directory for combined web summary: %s", output_path_for_summary)

            is_youtube = bool(shared_context.get("current_youtube_video_id"))
            all_chapters, yt_page_fname_with_subdir = self._assemble_all_web_chapters(
                shared_context, retrieved_data, is_yt_content=is_youtube
            )

            if not all_chapters:  # pragma: no cover
                self._log_warning("No chapters or content assembled. Skipping file writing.")
                shared_context["final_output_dir"] = str(output_path_for_summary)
                return False

            index_ctx = WebIndexContext(
                collection_name=str(retrieved_data.get("project_name", DEFAULT_WEB_COLLECTION_NAME_COMBINE)),
                web_relationships_data=cast(dict, retrieved_data.get("text_relationships", {})),
                footer_info=footer_obj,
                is_youtube_content=is_youtube,
                original_source_url=cast(Optional[str], retrieved_data.get("original_source_url")),
                chapter_files_data=all_chapters,
                youtube_page_filename=yt_page_fname_with_subdir,
            )
            index_md_content = self._prepare_web_index_content(index_ctx)
            footer_str = footer_obj.format_footer()

            if WEB_FOOTER_SEPARATOR not in index_md_content:
                index_md_content = index_md_content.rstrip() + f"\n{footer_str}"

            self._add_navigation_links(all_chapters, WEB_INDEX_FILENAME)
            self._add_footers(all_chapters, footer_str)

            write_success = self._write_all_files(output_path_for_summary, index_md_content, all_chapters, footer_str)
            if write_success:
                shared_context["final_output_dir"] = str(output_path_for_summary)
            elif shared_context.get("final_output_dir") == str(output_path_for_summary):  # pragma: no cover
                shared_context["final_output_dir"] = None
            return write_success

        except (ValueError, OSError, KeyError) as e_crit:  # pragma: no cover
            self._log_error("CombineWebSummary pre_execution failed: %s", e_crit, exc_info=True)
            if output_path_for_summary:
                self._log_warning("Attempted to write to output directory: %s.", output_path_for_summary.resolve())
            return False
        finally:
            self._log_info(">>> CombineWebSummary.pre_execution: Exit (write_success: %s). <<<", write_success)

    def execution(self, prepared_inputs: CombineWebSummaryPreparedInputs) -> CombineWebSummaryExecutionResult:
        """Indicate that the main work is done in pre_execution.

        Args:
            prepared_inputs: Result from pre_execution.

        Returns:
            Always None.
        """
        self._log_info("CombineWebSummary.execution: No direct action (pre_execution result: %s).", prepared_inputs)
        if not prepared_inputs:  # pragma: no cover
            self._log_warning("Execution: pre_execution step for CombineWebSummary indicated failure or no content.")
        return None

    def post_execution(
        self,
        shared_context: SLSharedContext,
        prepared_inputs: CombineWebSummaryPreparedInputs,
        execution_outputs: CombineWebSummaryExecutionResult,
    ) -> None:
        """Post-execution step for CombineWebSummary.

        Args:
            shared_context: The shared context dictionary.
            prepared_inputs: Result from pre_execution.
            execution_outputs: Result from execution (None).
        """
        del execution_outputs
        final_dir_val: Any = shared_context.get("final_output_dir")
        final_dir_str: str = str(final_dir_val) if final_dir_val else "Not set or N/A"
        status_msg: str

        if not prepared_inputs:  # pragma: no cover
            status_msg = "skipped by this node (no substantial content to combine or pre_execution failed)"
            final_dir_web_crawl_val: Any = shared_context.get("final_output_dir_web_crawl")
            if final_dir_web_crawl_val and isinstance(final_dir_web_crawl_val, str):
                final_dir_str = str(Path(final_dir_web_crawl_val).resolve())
                status_msg += f", using prior output dir: {final_dir_str}"
            else:
                final_dir_str = "Not set by any node"
        elif final_dir_val and Path(final_dir_str).exists():
            status_msg = "successfully (summary files written)"
            final_dir_str = str(Path(final_dir_str).resolve())
        else:  # pragma: no cover
            status_msg = "completed but output dir/files may have issues (check logs)"
            final_dir_str = "Potentially problematic or not set by this node"

        log_msg_l1 = f"CombineWebSummary.post_execution: Processing finished {status_msg}. "
        log_msg_l2 = f"Final output path considered by main: {final_dir_str}"
        self._log_info(log_msg_l1 + log_msg_l2)


# End of src/FL02_web_crawling/nodes/n08_combine_web_summary.py

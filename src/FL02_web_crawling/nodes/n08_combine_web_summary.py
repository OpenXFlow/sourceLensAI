# Copyright (C) 2025 Jozef Darida (LinkedIn/Xing)
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.

"""Node responsible for combining generated web content summary components into final files."""

import logging
import re
import sys
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Final, Literal, Optional, cast

from typing_extensions import TypeAlias

from sourcelens.core import BaseNode, SLSharedContext
from sourcelens.core.common_types import WebContentConceptsList, WebContentRelationshipsDict
from sourcelens.utils import LlmApiError
from sourcelens.utils.helpers import sanitize_filename

CombineWebSummaryPreparedInputs: TypeAlias = bool
CombineWebSummaryExecutionResult: TypeAlias = None

WebContentConceptItem: TypeAlias = dict[str, Any]
WebChapterOrderList: TypeAlias = list[int]
WebChapterContentList: TypeAlias = list[str]
WebChapterTypeLiteral: TypeAlias = Literal["standard", "inventory", "review"]
WebChapterFileData: TypeAlias = dict[str, Any]
AllWebChaptersDataList: TypeAlias = list[WebChapterFileData]
ConfigDictTyped: TypeAlias = dict[str, Any]
LlmConfigDictTyped: TypeAlias = dict[str, Any]
OutputConfigDictTyped: TypeAlias = dict[str, Any]
SharedDataForWebCombine: TypeAlias = dict[str, Any]

module_logger_combine_web: logging.Logger = logging.getLogger(__name__)

# Updated title for chunk-based inventory
WEB_INVENTORY_CHAPTER_TITLE: Final[str] = "Content Chunk Inventory"
WEB_INVENTORY_FILENAME_BASE: Final[str] = "content_inventory"  # Filename base can remain
WEB_REVIEW_CHAPTER_TITLE: Final[str] = "Content Review"
WEB_REVIEW_FILENAME_BASE: Final[str] = "content_review"
WEB_INDEX_FILENAME: Final[str] = "web_summary_index.md"
WEB_FOOTER_SEPARATOR: Final[str] = "\n\n---\n\n*Generated by"
WEB_PREV_LINK_REGEX: Final[re.Pattern[str]] = re.compile(
    r"^\s*(?:> ?)?Previously, we looked at\s+\[.*?\]\(.*?\)\.?\s*$",
    re.IGNORECASE | re.MULTILINE,
)
WEB_NEXT_LINK_REGEX: Final[re.Pattern[str]] = re.compile(
    r"^\s*(?:> ?)?Next, we will examine\s+\[.*?\]\(.*?\)\.?\s*$",
    re.IGNORECASE | re.MULTILINE,
)
WEB_H1_HEADING_REGEX: Final[re.Pattern[str]] = re.compile(r"^\s*#\s+.*$", re.MULTILINE)
DEFAULT_WEB_COLLECTION_NAME_COMBINE: Final[str] = "web_content_summary"
DEFAULT_OUTPUT_DIR_WEB_COMBINE: Final[str] = "output"
FOOTER_PROVIDER_DEFAULT_WEB: Final[str] = "N/A"
FOOTER_MODEL_DEFAULT_WEB: Final[str] = "N/A"
FOOTER_TARGET_LANG_DEFAULT_WEB: Final[str] = "english"
FILENAME_WEB_CHAPTER_PREFIX_WIDTH: Final[int] = 2
LOG_SNIPPET_LEN_WEB_COMBINE: Final[int] = 200
MAX_INT_FOR_SORTING_WEB: Final[int] = sys.maxsize
MINIMAL_INDEX_CONTENT_MARKER: Final[str] = "No chapters generated for this summary."


@dataclass(frozen=True)
class WebFooterInfo:
    """Encapsulate information for the web summary file footer."""

    provider_name: str
    model_name: str
    is_local: bool
    target_language: str

    def format_footer(self) -> str:
        """Generate the formatted footer string.

        Returns:
            str: The formatted footer string.
        """
        location: str = "(local)" if self.is_local else "(cloud)"
        repo_link: str = "https://github.com/openXFlow/sourceLensAI"  # Updated repo link
        part1: str = f"{WEB_FOOTER_SEPARATOR} [SourceLens AI]({repo_link}) "
        part2: str = f"using LLM: `{self.provider_name}` {location} "
        part3: str = f"- model: `{self.model_name}` | Target Language: `{self.target_language}`*"
        return part1 + part2 + part3


@dataclass(frozen=True)
class WebIndexContext:
    """Encapsulate context for the web summary index.md file."""

    collection_name: str
    web_relationships_data: WebContentRelationshipsDict
    footer_info: WebFooterInfo
    original_source_url: Optional[str] = None
    chapter_files_data: AllWebChaptersDataList = field(default_factory=list)


class CombineWebSummary(BaseNode[CombineWebSummaryPreparedInputs, CombineWebSummaryExecutionResult]):
    """Combine generated web content summary components into final Markdown files."""

    def _get_special_chapter_base_name(self, chapter_type: WebChapterTypeLiteral) -> str:
        """Return the base filename for a special web chapter type.

        Args:
            chapter_type: The type of the special chapter.

        Returns:
            The base filename string for that chapter type.
        """
        if chapter_type == "inventory":
            return WEB_INVENTORY_FILENAME_BASE
        if chapter_type == "review":
            return WEB_REVIEW_FILENAME_BASE
        self._log_warning("Unknown special web chapter type '%s' for filename base.", chapter_type)
        return "special_web_chapter"

    def _add_prev_link(self, chapter_data: WebChapterFileData, prev_link_text: Optional[str]) -> None:
        """Ensure the 'Previously...' navigation link exists once before H1 heading."""
        if not prev_link_text or not prev_link_text.strip():
            return
        content: str = str(chapter_data.get("content", "") or "")
        chapter_num_str = str(chapter_data.get("num", "Unknown"))
        content, num_removed = WEB_PREV_LINK_REGEX.subn("", content)
        if num_removed > 0:
            self._log_info("Removed %d 'Previously...' link(s) from web ch %s.", num_removed, chapter_num_str)
        content = re.sub(r"^\s*\n", "", content, flags=re.MULTILINE)
        content = re.sub(r"\n{3,}", "\n\n", content).strip()
        h1_match = WEB_H1_HEADING_REGEX.search(content)
        if h1_match:
            start_index = h1_match.start()
            prefix_newline = "\n" if start_index > 0 and not content[:start_index].isspace() else ""
            new_content_parts = [
                content[:start_index].rstrip(),
                prefix_newline,
                f"\n\n{prev_link_text}\n\n",
                content[start_index:],
            ]
            content = "".join(filter(None, new_content_parts))
        else:
            content = f"{prev_link_text}\n\n{content}"
            self._log_warning("No H1 in web ch %s. Added 'Previously...' link at start.", chapter_num_str)
        chapter_data["content"] = content.strip()

    def _add_next_link(self, chapter_data: WebChapterFileData, next_link_text: Optional[str]) -> None:
        """Ensure the 'Next...' navigation link exists once at the end of chapter content."""
        if not next_link_text or not next_link_text.strip():
            return
        content: str = str(chapter_data.get("content", "") or "")
        chapter_num_str = str(chapter_data.get("num", "Unknown"))
        content, num_removed = WEB_NEXT_LINK_REGEX.subn("", content)
        if num_removed > 0:
            self._log_info("Removed %d 'Next...' link(s) from web ch %s.", num_removed, chapter_num_str)
        content = re.sub(r"\n{3,}", "\n\n", content).strip()
        content += f"\n\n{next_link_text}" if content else next_link_text
        chapter_data["content"] = content.strip()

    def _add_navigation_links(self, all_chapters_data: AllWebChaptersDataList, index_filename: str) -> None:
        """Add or update 'Previously...' and 'Next...' links in all web chapters.

        Args:
            all_chapters_data: List of all chapter data dictionaries.
            index_filename: The filename of the main index/overview page.
        """
        num_all_chapters = len(all_chapters_data)
        if num_all_chapters == 0:
            self._log_warning("No web chapters found to add navigation links.")
            return
        for i, chapter_data in enumerate(all_chapters_data):
            prev_link_text: Optional[str] = None
            if i == 0:
                prev_link_text = f"> Previously, we looked at the [Content Overview]({index_filename})."
            elif i > 0:
                prev_ch = all_chapters_data[i - 1]
                p_name = str(prev_ch.get("name", "")).strip()
                p_file = str(prev_ch.get("filename", "")).strip()
                if p_name and p_file:
                    prev_link_text = f"> Previously, we looked at [{p_name}]({p_file})."
                else:
                    self._log_warning("Could not create 'previous' link for web ch %s.", chapter_data.get("num", i + 1))
            self._add_prev_link(chapter_data, prev_link_text)

            next_link_text: Optional[str] = None
            if i < num_all_chapters - 1:
                next_ch = all_chapters_data[i + 1]
                n_name = str(next_ch.get("name", "")).strip()
                n_file = str(next_ch.get("filename", "")).strip()
                if n_name and n_file:
                    next_link_text = f"> Next, we will examine [{n_name}]({n_file})."
                else:
                    self._log_warning("Could not create 'next' link for web ch %s.", chapter_data.get("num", i + 1))
            self._add_next_link(chapter_data, next_link_text)

    def _add_footers(self, all_chapters_data: AllWebChaptersDataList, footer_text: str) -> None:
        """Append the standard footer to all web chapter contents.

        Args:
            all_chapters_data: List of all chapter data dictionaries.
            footer_text: The footer text to append.
        """
        clean_footer_text: str = footer_text.strip()
        if not clean_footer_text:
            self._log_warning("Footer text is empty, not adding footers to web chapters.")
            return
        for chapter_data in all_chapters_data:
            current_content: str = str(chapter_data.get("content", "") or "")
            if WEB_FOOTER_SEPARATOR in current_content:
                current_content = current_content.split(WEB_FOOTER_SEPARATOR, 1)[0]
            current_content = current_content.rstrip()
            chapter_data["content"] = f"{current_content}\n{footer_text}" if current_content else footer_text.lstrip()

    def _prepare_web_index_content(self, context: WebIndexContext) -> str:
        """Prepare the full Markdown content for the web summary index.md file.

        Args:
            context: Context object with all necessary data.

        Returns:
            The complete Markdown content for the index file.
        """
        self._log_info("Preparing web summary index.md content for '%s'...", context.collection_name)
        summary_raw: Any = context.web_relationships_data.get(
            "overall_summary", f"Summary of content from {context.collection_name}."
        )
        summary: str = str(summary_raw or f"Summary from {context.collection_name}.")
        content_parts: list[str] = [f"# Web Content Summary: {context.collection_name}\n\n{summary}\n"]
        if context.original_source_url:
            content_parts.append(
                f"**Original Source:** [{context.original_source_url}]({context.original_source_url})\n"
            )

        content_parts.append("## Content Overview (Chapters)\n")
        sorted_chapters = sorted(context.chapter_files_data, key=lambda x: x.get("num", MAX_INT_FOR_SORTING_WEB))
        chapter_links: list[str] = []
        for ch_data in sorted_chapters:
            num, name, fname = ch_data.get("num"), ch_data.get("name"), ch_data.get("filename")
            if (
                isinstance(num, int)
                and num > 0
                and isinstance(name, str)
                and name.strip()
                and isinstance(fname, str)
                and fname.strip()
            ):
                chapter_links.append(f"{num}. [{name.strip()}]({fname.strip()})")
        chapter_list_str = "\n".join(chapter_links) if chapter_links else MINIMAL_INDEX_CONTENT_MARKER
        content_parts.append(chapter_list_str)
        return "\n".join(content_parts)

    def _prepare_standard_web_chapters_data(
        self,
        concepts: WebContentConceptsList,
        chapter_order: WebChapterOrderList,
        chapters_content: WebChapterContentList,
    ) -> AllWebChaptersDataList:
        """Prepare initial data structure for standard web chapter files.

        Args:
            concepts: List of identified web concepts.
            chapter_order: Ordered list of concept indices.
            chapters_content: List of generated Markdown for each chapter.

        Returns:
            A list of dictionaries, each representing a standard chapter.
        """
        std_chapters: AllWebChaptersDataList = []
        num_concepts = len(concepts)
        effective_count = min(len(chapter_order), len(chapters_content))

        if len(chapter_order) != len(chapters_content):
            self._log_warning(
                "Web chapter order count (%d) != content count (%d). Processing %d.",
                len(chapter_order),
                len(chapters_content),
                effective_count,
            )

        for i in range(effective_count):
            concept_idx = chapter_order[i]
            if not (0 <= concept_idx < num_concepts):
                self._log_warning("Invalid concept_idx %d in web chapter order. Skipping.", concept_idx)
                continue
            concept_item: WebContentConceptItem = concepts[concept_idx]
            name_raw: Any = concept_item.get("name", f"Topic {i + 1}")
            name: str = str(name_raw).strip() if isinstance(name_raw, str) and name_raw.strip() else f"Topic {i + 1}"

            std_chapters.append(
                {
                    "content": str(chapters_content[i] or ""),
                    "name": name,
                    "concept_index": concept_idx,
                    "chapter_type": "standard",
                    "filename": "",
                    "num": -1,
                }
            )
        self._log_info("Prepared data for %d standard web chapters.", len(std_chapters))
        return std_chapters

    def _prepare_special_web_chapter_data(
        self, content_key: str, title: str, chapter_type: WebChapterTypeLiteral, shared_context: SLSharedContext
    ) -> Optional[WebChapterFileData]:
        """Prepare data structure for a special web chapter (inventory, review).

        Args:
            content_key: Key in shared_context holding the chapter's Markdown content.
            title: The title of this special chapter.
            chapter_type: The type of the special chapter.
            shared_context: The shared context dictionary.

        Returns:
            A dictionary with chapter data if content is found, else None.
        """
        content_val_any: Any = shared_context.get(content_key)
        if not content_val_any or not isinstance(content_val_any, str) or not content_val_any.strip():
            self._log_info("No valid content for '%s' web chapter (key: '%s').", chapter_type, content_key)
            return None
        content_val_str: str = content_val_any
        self._log_info(
            "'%s' web chapter data prepared (content length: %d). Title: %s",
            chapter_type.capitalize(),
            len(content_val_str),
            title,
        )
        return {
            "filename": "",
            "content": content_val_str,
            "name": title,
            "num": -1,
            "concept_index": -1,  # Not applicable for special chapters
            "chapter_type": chapter_type,
        }

    def _assemble_all_web_chapters(
        self, shared_context: SLSharedContext, data: SharedDataForWebCombine
    ) -> AllWebChaptersDataList:
        """Assemble standard and special web chapters, then number and name files.

        Args:
            shared_context: The main shared context.
            data: Data retrieved specifically for this node.

        Returns:
            A list of all chapter data, sorted and with filenames.
        """
        all_chapters: AllWebChaptersDataList = []
        flow_specific_output_options: OutputConfigDictTyped = cast(
            OutputConfigDictTyped, shared_context.get("current_mode_output_options", {})
        )

        concepts: WebContentConceptsList = cast(WebContentConceptsList, data.get("text_concepts", []))
        order: WebChapterOrderList = cast(WebChapterOrderList, data.get("text_chapter_order", []))
        content_list: WebChapterContentList = cast(WebChapterContentList, data.get("text_chapters", []))
        all_chapters.extend(self._prepare_standard_web_chapters_data(concepts, order, content_list))

        for i, chap_data in enumerate(all_chapters):
            chap_data["num"] = i + 1
        current_chapter_num = len(all_chapters) + 1

        if bool(flow_specific_output_options.get("include_content_inventory", False)):
            inventory_chapter = self._prepare_special_web_chapter_data(
                "content_inventory_md", WEB_INVENTORY_CHAPTER_TITLE, "inventory", shared_context
            )
            if inventory_chapter:
                inventory_chapter["num"] = current_chapter_num
                all_chapters.append(inventory_chapter)
                current_chapter_num += 1

        if bool(flow_specific_output_options.get("include_content_review", False)):
            review_chapter = self._prepare_special_web_chapter_data(
                "web_content_review_md", WEB_REVIEW_CHAPTER_TITLE, "review", shared_context
            )
            if review_chapter:
                review_chapter["num"] = current_chapter_num
                all_chapters.append(review_chapter)
                # current_chapter_num += 1 # No need to increment if it's the last one

        all_chapters.sort(key=lambda x: x.get("num", MAX_INT_FOR_SORTING_WEB))
        # Re-assign numbers after sorting to ensure they are consecutive if any special chapter was skipped
        for i, chapter_info in enumerate(all_chapters):
            chapter_info["num"] = i + 1
            name_raw = str(chapter_info.get("name", f"web-chapter-{chapter_info['num']}"))
            ch_type = cast(WebChapterTypeLiteral, chapter_info.get("chapter_type", "standard"))
            base_name = (
                self._get_special_chapter_base_name(ch_type)
                if ch_type in ["inventory", "review"]
                else (sanitize_filename(name_raw) or f"chapter-{chapter_info['num']}")
            )
            num_val = cast(int, chapter_info["num"])
            chapter_info["filename"] = f"{num_val:0{FILENAME_WEB_CHAPTER_PREFIX_WIDTH}d}_{base_name}.md"

        self._log_info("Assembled and numbered %d total web chapters.", len(all_chapters))
        return all_chapters

    def _retrieve_shared_data_for_web_combine(self, shared_context: SLSharedContext) -> SharedDataForWebCombine:
        """Retrieve all necessary data from shared context for web summary combination.

        Args:
            shared_context: The main shared context.

        Returns:
            A dictionary of required data.

        Raises:
            ValueError: If essential data is missing from shared_context.
        """
        self._logger.debug("--- Retrieving data from shared_context for CombineWebSummary ---")
        data: SharedDataForWebCombine = {
            "project_name": str(self._get_required_shared(shared_context, "project_name")),
            "output_base_dir": str(self._get_required_shared(shared_context, "output_dir")),
            "text_concepts": self._get_required_shared(shared_context, "text_concepts"),
            "text_relationships": self._get_required_shared(shared_context, "text_relationships"),
            "text_chapter_order": self._get_required_shared(shared_context, "text_chapter_order"),
            "text_chapters": self._get_required_shared(shared_context, "text_chapters"),
            "content_inventory_md": shared_context.get("content_inventory_md"),
            "web_content_review_md": shared_context.get("web_content_review_md"),
            "llm_config": self._get_required_shared(shared_context, "llm_config"),
            "config": self._get_required_shared(shared_context, "config"),
            "original_source_url": shared_context.get("crawl_url")
            or shared_context.get("crawl_sitemap")
            or shared_context.get("crawl_file"),
        }
        self._logger.debug("--- Finished retrieving data from shared_context for web summary ---")
        return data

    def _initialize_output_context(
        self, shared_context: SLSharedContext, retrieved_data: SharedDataForWebCombine
    ) -> tuple[WebFooterInfo, Path]:
        """Initialize footer info and determine the output path.

        Args:
            shared_context: The main shared context.
            retrieved_data: Data specific to this node.

        Returns:
            Footer information and the resolved output path.
        """
        llm_cfg_val: Any = retrieved_data.get("llm_config", {})
        llm_cfg: LlmConfigDictTyped = cast(LlmConfigDictTyped, llm_cfg_val)
        target_lang_val: Any = shared_context.get("language", FOOTER_TARGET_LANG_DEFAULT_WEB)

        footer_obj = WebFooterInfo(
            provider_name=str(llm_cfg.get("provider", FOOTER_PROVIDER_DEFAULT_WEB)),
            model_name=str(llm_cfg.get("model", FOOTER_MODEL_DEFAULT_WEB)),
            is_local=bool(llm_cfg.get("is_local_llm", False)),
            target_language=str(target_lang_val),
        )

        project_name_str = str(retrieved_data.get("project_name", DEFAULT_WEB_COLLECTION_NAME_COMBINE))
        safe_project_name = (
            sanitize_filename(project_name_str, allow_underscores=False) or DEFAULT_WEB_COLLECTION_NAME_COMBINE
        )

        output_base_dir_str = str(retrieved_data.get("output_base_dir", DEFAULT_OUTPUT_DIR_WEB_COMBINE))
        output_path_str_from_context = shared_context.get("final_output_dir_web_crawl")

        if output_path_str_from_context and isinstance(output_path_str_from_context, str):
            output_path = Path(output_path_str_from_context)
        else:
            current_flow_name = str(shared_context.get("current_operation_mode", "FL02_web_crawling"))
            config_val_prep: Any = shared_context.get("config", {})
            flow_settings: dict[str, Any] = (
                config_val_prep.get(current_flow_name, {}) if isinstance(config_val_prep, dict) else {}
            )
            crawler_options_prep: dict[str, Any] = flow_settings.get("crawler_options", {})
            default_subdir = str(crawler_options_prep.get("default_output_subdir_name", "crawled_web_content"))
            output_path = Path(output_base_dir_str) / default_subdir / safe_project_name
        return footer_obj, output_path.resolve()

    def _write_all_files(
        self, output_path: Path, index_content: str, all_chapters: AllWebChaptersDataList, footer_str: str
    ) -> bool:
        """Write index and chapter files to disk.

        Args:
            output_path: The base output directory for this summary.
            index_content: The Markdown content for the index file.
            all_chapters: List of all chapter data.
            footer_str: The footer string to append if not present.

        Returns:
            True if all writes were successful, False otherwise.
        """
        try:
            output_path.mkdir(parents=True, exist_ok=True)
            self._log_info("Ensured web summary output directory exists: %s", output_path)
        except OSError as e_dir:
            self._log_error("CRITICAL: Failed to create web summary output directory %s: %s", output_path, e_dir)
            return False

        index_filepath = output_path / WEB_INDEX_FILENAME
        final_index_content = index_content.rstrip()
        if WEB_FOOTER_SEPARATOR not in final_index_content:
            final_index_content += f"\n{footer_str}"

        try:
            index_filepath.write_text(final_index_content, encoding="utf-8")
            self._log_info("Wrote web summary index file: %s", index_filepath)
        except OSError as e_idx:
            self._log_error("CRITICAL: Failed to write web summary index file %s: %s", index_filepath, e_idx)
            return False

        all_chapter_writes_ok = True
        for ch_info in all_chapters:
            fname = str(ch_info.get("filename", ""))
            content = str(ch_info.get("content", ""))
            if fname and content:  # Ensure content is also not empty
                try:
                    (output_path / fname).write_text(content, encoding="utf-8")
                    self._log_info("Wrote web chapter: %s", fname)
                except OSError as e_ch:
                    self._log_error("Failed to write web chapter file %s: %s", fname, e_ch)
                    all_chapter_writes_ok = False
            else:
                self._log_warning(
                    "Skipping chapter write due to missing filename or empty content: %s", ch_info.get("name")
                )
        return all_chapter_writes_ok

    def pre_execution(self, shared_context: SLSharedContext) -> CombineWebSummaryPreparedInputs:
        """Prepare all web summary content and write output files.

        Args:
            shared_context: The shared context dictionary.

        Returns:
            True if successful, False otherwise.
        """
        self._log_info(">>> CombineWebSummary.pre_execution: Assembling and writing web summary files. <<<")
        output_path_for_summary: Optional[Path] = None
        write_success = False
        try:
            retrieved_data = self._retrieve_shared_data_for_web_combine(shared_context)
            footer_obj, output_path_for_summary = self._initialize_output_context(shared_context, retrieved_data)
            self._log_info("Final output directory for web summary: %s", output_path_for_summary)

            all_chapters = self._assemble_all_web_chapters(shared_context, retrieved_data)

            index_ctx = WebIndexContext(
                collection_name=str(retrieved_data.get("project_name", DEFAULT_WEB_COLLECTION_NAME_COMBINE)),
                web_relationships_data=cast(WebContentRelationshipsDict, retrieved_data.get("text_relationships", {})),
                footer_info=footer_obj,
                original_source_url=cast(Optional[str], retrieved_data.get("original_source_url")),
                chapter_files_data=all_chapters,
            )
            index_md_content = self._prepare_web_index_content(index_ctx)
            footer_str = footer_obj.format_footer()

            is_index_trivial = MINIMAL_INDEX_CONTENT_MARKER in index_md_content

            if not all_chapters and is_index_trivial:
                self._log_info(
                    "No meaningful summary content (no chapters and index is trivial). "
                    "CombineWebSummary will not create its own output files or set final_output_dir."
                )
                return False

            self._add_navigation_links(all_chapters, WEB_INDEX_FILENAME)
            self._add_footers(all_chapters, footer_str)

            write_success = self._write_all_files(output_path_for_summary, index_md_content, all_chapters, footer_str)
            if write_success:
                shared_context["final_output_dir"] = str(output_path_for_summary)
            else:
                if shared_context.get("final_output_dir") == str(output_path_for_summary):
                    shared_context["final_output_dir"] = None
            return write_success

        except (ValueError, LlmApiError, OSError) as e_crit:
            self._log_error("CombineWebSummary pre_execution critical error: %s", e_crit, exc_info=True)
            if output_path_for_summary:
                self._log_warning(
                    "Attempted to write to output directory: %s. Check logs.", output_path_for_summary.resolve()
                )
            return False
        finally:
            self._log_info(">>> CombineWebSummary.pre_execution: Exit (write_success: %s). <<<", write_success)

    def execution(self, prepared_inputs: CombineWebSummaryPreparedInputs) -> CombineWebSummaryExecutionResult:
        """Indicate that the main work is done in pre_execution.

        Args:
            prepared_inputs: Result from pre_execution.

        Returns:
            Always None.
        """
        self._log_info("CombineWebSummary.execution: No direct action (pre_execution result: %s).", prepared_inputs)
        if not prepared_inputs:
            self._log_warning("Execution: pre_execution step for CombineWebSummary indicated failure.")
        return None

    def post_execution(
        self,
        shared_context: SLSharedContext,
        prepared_inputs: CombineWebSummaryPreparedInputs,
        execution_outputs: CombineWebSummaryExecutionResult,
    ) -> None:
        """Post-execution step for CombineWebSummary.

        Args:
            shared_context: The shared context dictionary.
            prepared_inputs: Result from pre_execution.
            execution_outputs: Result from execution (None).
        """
        del execution_outputs
        final_dir_val: Any = shared_context.get("final_output_dir")
        final_dir_str: str = str(final_dir_val) if final_dir_val else "Not set or N/A"
        status_msg: str

        if not prepared_inputs:
            status_msg = "skipped by this node (no substantial content to combine)"
            final_dir_web_crawl_val: Any = shared_context.get("final_output_dir_web_crawl")
            if final_dir_web_crawl_val and isinstance(final_dir_web_crawl_val, str):
                final_dir_str = str(Path(final_dir_web_crawl_val).resolve())
                status_msg += f", using raw crawl output dir: {final_dir_str}"
            else:
                final_dir_str = "Not set by any node"
        elif final_dir_val and Path(final_dir_str).exists():
            status_msg = "successfully (summary files written)"
            final_dir_str = str(Path(final_dir_str).resolve())
        else:
            status_msg = "completed but output dir/files may have issues (check logs)"
            final_dir_str = "Potentially problematic or not set by this node"

        log_msg_l1 = f"CombineWebSummary.post_execution: Processing finished {status_msg}. "
        log_msg_l2 = f"Final output path considered by main: {final_dir_str}"
        self._log_info(log_msg_l1 + log_msg_l2)


# End of src/FL02_web_crawling/nodes/n08_combine_web_summary.py

# Copyright (C) 2025 Jozef Darida (LinkedIn/Xing)
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.

"""Node responsible for generating an AI-powered project review."""

from typing import TYPE_CHECKING, Any, Final, Optional

from typing_extensions import TypeAlias

from sourcelens.core import BaseNode, SLSharedContext

if TYPE_CHECKING:
    from sourcelens.core.common_types import (
        CodeAbstractionsList,
        CodeRelationshipsDict,
        FilePathContentList,
    )
from sourcelens.utils.llm_api import LlmApiError, call_llm
from sourcelens.utils.validation import ValidationFailure, validate_yaml_dict

from ..prompts.project_review_prompts import (
    PROJECT_REVIEW_SCHEMA,
    ProjectReviewPrompts,
)

if TYPE_CHECKING:  # pragma: no cover
    ResolvedLlmConfigDict: TypeAlias = dict[str, Any]
    ResolvedCacheConfigDict: TypeAlias = dict[str, Any]


ProjectReviewPreparedInputs: TypeAlias = dict[str, Any]
ProjectReviewExecutionResult: TypeAlias = Optional[str]

# Removed local TypeAliases:
# AbstractionsListInternal
# RelationshipsDictInternal
# FileDataInternal
# FileDataListInternal
# These will be replaced by CodeAbstractionsList, CodeRelationshipsDict, FilePathContentList

MAX_REVIEW_SNIPPET_LEN_LOG: Final[int] = 200
EXPECTED_FILE_DATA_ITEM_LENGTH: Final[int] = 2


class GenerateProjectReview(BaseNode[ProjectReviewPreparedInputs, ProjectReviewExecutionResult]):
    """Generate an AI-powered project review based on analyzed project data.

    This node takes identified abstractions, their relationships, and file data
    to prompt an LLM for a structured project review. The review includes key
    characteristics, areas for discussion, observed patterns, and an overall
    summary. The LLM's YAML output is validated and then formatted into Markdown.
    """

    def _format_review_yaml_to_markdown(self, review_data: dict[str, Any], project_name: str) -> str:
        """Format the structured YAML data from LLM into Markdown.

        Args:
            review_data: The validated dictionary parsed from LLM's YAML response.
                         Expected to conform to `PROJECT_REVIEW_SCHEMA`.
            project_name: The name of the project, used in the review title.

        Returns:
            A string containing the formatted Markdown for the project review chapter.
        """
        markdown_parts: list[str] = [f"# Project Review: {project_name}\n"]
        warning_text_l1 = (
            "> **Note:** This review is automatically generated by an AI (Large Language Model) "
            "based on an analysis of the project's abstractions, "
        )
        warning_text_l2 = (
            "relationships, and file structure. "
            "It is intended to provide high-level insights and stimulate discussion, "
            "not as a definitive expert assessment. "
            "Always use critical judgment when interpreting AI-generated content."
        )
        markdown_parts.append(f"{warning_text_l1}{warning_text_l2}\n")

        summary: str = str(review_data.get("overall_summary", "No overall summary provided."))
        markdown_parts.append(f"## AI-Generated Overall Summary\n\n{summary}\n")

        key_chars_any: Any = review_data.get("key_characteristics", [])
        key_chars: list[str] = (
            key_chars_any if isinstance(key_chars_any, list) and all(isinstance(i, str) for i in key_chars_any) else []
        )
        if key_chars:
            markdown_parts.append("## Key Architectural Characteristics (AI-Observed)\n")
            for char_item in key_chars:
                markdown_parts.append(f"- {char_item}")
            markdown_parts.append("\n")

        areas_disc_any: Any = review_data.get("areas_for_discussion", [])
        areas_disc: list[str] = (
            areas_disc_any
            if isinstance(areas_disc_any, list) and all(isinstance(i, str) for i in areas_disc_any)
            else []
        )
        if areas_disc:
            markdown_parts.append("## Potential Areas for Discussion (AI-Suggested)\n")
            for area_item in areas_disc:
                markdown_parts.append(f"- {area_item}")
            markdown_parts.append("\n")

        obs_patterns_any: Any = review_data.get("observed_patterns", [])
        obs_patterns: list[str] = (
            obs_patterns_any
            if isinstance(obs_patterns_any, list) and all(isinstance(i, str) for i in obs_patterns_any)
            else []
        )
        if obs_patterns:
            markdown_parts.append("## Observed Patterns & Structural Notes (AI-Identified)\n")
            for pattern_item in obs_patterns:
                markdown_parts.append(f"- {pattern_item}")
            markdown_parts.append("\n")

        coding_obs_any: Any = review_data.get("coding_practice_observations", [])
        coding_obs: list[str] = (
            coding_obs_any
            if isinstance(coding_obs_any, list) and all(isinstance(i, str) for i in coding_obs_any)
            else []
        )
        if coding_obs:
            markdown_parts.append("## Coding Practice Observations (AI-Noted)\n")
            for obs_item in coding_obs:
                markdown_parts.append(f"- {obs_item}")
            markdown_parts.append("\n")

        return "\n".join(markdown_parts)

    def pre_execution(self, shared_context: SLSharedContext) -> ProjectReviewPreparedInputs:
        """Prepare necessary data and context for generating the project review.

        Retrieves abstractions, relationships, file data, project details, and
        relevant LLM/cache configurations from `shared_context`.
        Determines if project review generation should be skipped based on config.

        Args:
            shared_context: The shared context dictionary. Expected to contain:
                            "abstractions", "relationships", "files", "project_name",
                            "config" (full resolved config), "language",
                            "llm_config" (resolved for current mode),
                            "cache_config" (common cache settings),
                            "current_mode_output_options".

        Returns:
            A dictionary containing data for the `execution` method, or
            `{"skip": True, "reason": ...}` if prerequisites are not met
            or review generation is disabled.

        Raises:
            ValueError: If essential data like 'config' or specific sub-keys are
                        missing or of an unexpected type in `shared_context`.
        """
        self._log_info("Preparing for project review generation.")
        try:
            current_mode_opts_any: Any = shared_context.get("current_mode_output_options", {})
            current_mode_opts: dict[str, Any] = current_mode_opts_any if isinstance(current_mode_opts_any, dict) else {}

            include_review_val: Any = current_mode_opts.get("include_project_review")
            include_review: bool = include_review_val if isinstance(include_review_val, bool) else False

            if not include_review:
                self._log_info("Project review generation is disabled in configuration. Skipping.")
                return {"skip": True, "reason": "Disabled via 'output_options.include_project_review'"}

            abstractions_data_any: Any = self._get_required_shared(shared_context, "abstractions")
            relationships_data_any: Any = self._get_required_shared(shared_context, "relationships")
            files_data_any: Any = self._get_required_shared(shared_context, "files")
            project_name_val: Any = self._get_required_shared(shared_context, "project_name")
            language_val: Any = shared_context.get("language", "unknown")
            llm_config_val: Any = self._get_required_shared(shared_context, "llm_config")
            cache_config_val: Any = self._get_required_shared(shared_context, "cache_config")

            # Use CodeAbstractionsList
            abstractions_data: CodeAbstractionsList = []
            if isinstance(abstractions_data_any, list):
                abstractions_data = [item for item in abstractions_data_any if isinstance(item, dict)]
            if len(abstractions_data) != len(abstractions_data_any or []):
                self._log_warning(
                    "Some items in 'abstractions' were not dictionaries or did not match expected structure."
                )

            # Use CodeRelationshipsDict
            relationships_data: CodeRelationshipsDict = (
                relationships_data_any if isinstance(relationships_data_any, dict) else {}
            )
            files_data_raw: list[Any] = files_data_any if isinstance(files_data_any, list) else []
            # Use FilePathContentList
            files_data: FilePathContentList = []
            for item in files_data_raw:
                if (
                    isinstance(item, tuple)
                    and len(item) == EXPECTED_FILE_DATA_ITEM_LENGTH
                    and isinstance(item[0], str)
                    and isinstance(item[1], str)  # Assuming content is always string here
                ):
                    files_data.append(item)
                else:
                    self._log_warning("Skipping invalid item in files_data for project review context: %s", item)

            prepared_inputs: ProjectReviewPreparedInputs = {
                "skip": False,
                "project_name": str(project_name_val),
                "abstractions_data": abstractions_data,
                "relationships_data": relationships_data,
                "files_data": files_data,
                "language": str(language_val),
                "llm_config": llm_config_val if isinstance(llm_config_val, dict) else {},
                "cache_config": cache_config_val if isinstance(cache_config_val, dict) else {},
            }
            return prepared_inputs
        except ValueError as e_prep_val:
            self._log_error("Preparation for project review failed due to missing/invalid data: %s", e_prep_val)
            return {"skip": True, "reason": f"Data preparation error: {e_prep_val!s}"}
        except (KeyError, TypeError) as e_struct:
            self._log_error("Error accessing config structure during project review pre_execution: %s", e_struct)
            return {"skip": True, "reason": f"Configuration structure error: {e_struct!s}"}

    def execution(self, prepared_inputs: ProjectReviewPreparedInputs) -> ProjectReviewExecutionResult:
        """Generate the project review using an LLM.

        If an `LlmApiError` occurs, it is re-raised to be handled by the `Node`'s
        retry logic (if configured). Other errors during validation or processing
        result in a fallback Markdown message being returned.

        Args:
            prepared_inputs: The dictionary returned by the `pre_execution` method.
                             Expected to contain all necessary data for prompting the LLM.

        Returns:
            A string containing the Markdown content for the project review,
            or None if execution was skipped. If LLM call or validation fails
            after retries (handled by Node), this method's `execution_fallback`
            will be invoked by the `Node`'s internal logic.

        Raises:
            LlmApiError: If the LLM API call fails, to allow for retries.
        """
        if prepared_inputs.get("skip", True):
            reason_val: Any = prepared_inputs.get("reason", "N/A")
            self._log_info("Skipping project review execution. Reason: %s", str(reason_val))
            return None

        project_name: str = prepared_inputs["project_name"]
        self._log_info("Generating project review for '%s' using LLM...", project_name)

        abstractions_data: CodeAbstractionsList = prepared_inputs["abstractions_data"]
        relationships_data: CodeRelationshipsDict = prepared_inputs["relationships_data"]
        files_data: FilePathContentList = prepared_inputs["files_data"]
        language: str = prepared_inputs["language"]
        llm_config: "ResolvedLlmConfigDict" = prepared_inputs["llm_config"]
        cache_config: "ResolvedCacheConfigDict" = prepared_inputs["cache_config"]

        prompt = ProjectReviewPrompts.format_project_review_prompt(
            project_name=project_name,
            abstractions_data=abstractions_data,
            relationships_data=relationships_data,
            files_data=files_data,
            language=language,
        )

        try:
            response_text = call_llm(prompt, llm_config, cache_config)
            validated_yaml_data = validate_yaml_dict(response_text, PROJECT_REVIEW_SCHEMA)
            markdown_content = self._format_review_yaml_to_markdown(validated_yaml_data, project_name)
            self._log_info("Successfully generated project review content.")
            return markdown_content
        except LlmApiError:
            self._log_error(
                "LLM call failed during project review generation. This error will be re-raised for retry/fallback."
            )
            raise
        except ValidationFailure as e_val:
            self._log_error("YAML validation failed for project review: %s", e_val)
            return f"# Project Review: {project_name}\n\n> AI-generated review validation failed: {e_val!s}"
        except (ValueError, TypeError, AttributeError, KeyError) as e_proc:
            self._log_error("Unexpected error processing project review: %s", e_proc, exc_info=True)
            return f"# Project Review: {project_name}\n\n> Unexpected error generating review: {e_proc!s}"

    def execution_fallback(
        self, prepared_inputs: ProjectReviewPreparedInputs, exc: Exception
    ) -> ProjectReviewExecutionResult:
        """Handle fallback if all execution attempts for project review fail.

        This method is called by the parent `Node` class's retry mechanism if all
        attempts to call `self.execution()` (which internally calls the LLM) fail
        due to recoverable errors (like `LlmApiError`).

        Args:
            prepared_inputs: The data from the `pre_execution` phase, which was
                             passed to the failed `execution` attempts.
            exc: The exception that occurred during the final execution attempt
                 (typically an `LlmApiError`).

        Returns:
            A Markdown string indicating the failure to generate the project review.
        """
        project_name_val: Any = prepared_inputs.get("project_name", "Unknown Project")
        project_name: str = str(project_name_val)
        self._log_error(
            "All attempts to generate project review for '%s' failed. Last error: %s", project_name, exc, exc_info=True
        )
        return (
            f"# Project Review: {project_name}\n\n"
            f"> AI-generated review could not be created after multiple attempts. Error: {exc!s}"
        )

    def post_execution(
        self,
        shared_context: SLSharedContext,
        prepared_inputs: ProjectReviewPreparedInputs,
        execution_outputs: ProjectReviewExecutionResult,
    ) -> None:
        """Store the generated project review content in shared context.

        Args:
            shared_context: The shared context dictionary to update.
            prepared_inputs: Result from the `pre_execution` phase. Used here to
                             check if execution was skipped.
            execution_outputs: Markdown content of the project review, or None if
                               skipped, or an error string if generation failed.
        """
        if prepared_inputs.get("skip", True):
            shared_context["project_review_content"] = None
            self._log_info("Project review was skipped, 'project_review_content' set to None.")
            return

        shared_context["project_review_content"] = execution_outputs
        project_name = str(prepared_inputs.get("project_name", "Unknown Project"))
        error_message_start_heuristic = f"# Project Review: {project_name}\n\n> AI-generated review"

        if (
            execution_outputs
            and execution_outputs.strip()
            and not execution_outputs.startswith(error_message_start_heuristic)
        ):
            snippet = execution_outputs[:MAX_REVIEW_SNIPPET_LEN_LOG].replace("\n", " ")
            if len(execution_outputs) > MAX_REVIEW_SNIPPET_LEN_LOG:
                snippet += "..."
            self._log_info("Stored project review content in shared context (snippet: '%s').", snippet)
        elif execution_outputs:
            self._log_warning("Project review content from execution was an error message or empty. Stored as is.")
        else:
            self._log_warning("Project review content from execution was None. Stored None.")


# End of src/FL01_code_analysis/nodes/n09_generate_project_review.py

# Copyright (C) 2025 Jozef Darida (LinkedIn/Xing)
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.

"""Node responsible for generating an AI-powered project review."""

from typing import Any, Final, Optional

from typing_extensions import TypeAlias

from sourcelens.nodes.base_node import BaseNode, SLSharedContext
from sourcelens.prompts.code.project_review_prompts import ProjectReviewPrompts
from sourcelens.utils.llm_api import LlmApiError, call_llm
from sourcelens.utils.validation import ValidationFailure, validate_yaml_dict

ProjectReviewPreparedInputs: TypeAlias = dict[str, Any]
ProjectReviewExecutionResult: TypeAlias = Optional[str]

AbstractionsListInternal: TypeAlias = list[dict[str, Any]]
RelationshipsDictInternal: TypeAlias = dict[str, Any]
FileDataListInternal: TypeAlias = list[tuple[str, str]]
LlmConfigDictInternal: TypeAlias = dict[str, Any]
CacheConfigDictInternal: TypeAlias = dict[str, Any]
ConfigDictInternal: TypeAlias = dict[str, Any]
OutputConfigDictInternal: TypeAlias = dict[str, Any]

MAX_REVIEW_SNIPPET_LEN: Final[int] = 200
PROJECT_REVIEW_SCHEMA: Final[dict[str, Any]] = {
    "type": "object",
    "properties": {
        "key_characteristics": {"type": "array", "items": {"type": "string"}},
        "areas_for_discussion": {"type": "array", "items": {"type": "string"}},
        "observed_patterns": {"type": "array", "items": {"type": "string"}},
        "coding_practice_observations": {"type": "array", "items": {"type": "string"}},
        "overall_summary": {"type": "string"},
    },
    "required": ["key_characteristics", "areas_for_discussion", "observed_patterns", "overall_summary"],
    "additionalProperties": False,
}


class GenerateProjectReview(BaseNode[ProjectReviewPreparedInputs, ProjectReviewExecutionResult]):
    """Generate an AI-powered project review based on analyzed project data."""

    def _format_review_yaml_to_markdown(self, review_data: dict[str, Any], project_name: str) -> str:
        """Format the structured YAML data from LLM into Markdown.

        Args:
            review_data: The validated dictionary parsed from LLM's YAML response.
            project_name: The name of the project.

        Returns:
            A string containing the formatted Markdown for the project review chapter.
        """
        markdown_parts: list[str] = [f"# Project Review: {project_name}\n"]
        warning_text_l1 = (
            "> **Note:** This review is automatically generated by an AI (Large Language Model) "
            "based on an analysis of the project's abstractions, "
        )
        warning_text_l2 = (
            "relationships, and file structure. "
            "It is intended to provide high-level insights and stimulate discussion, "
            "not as a definitive expert assessment. "
            "Always use critical judgment when interpreting AI-generated content."
        )
        markdown_parts.append(f"{warning_text_l1}{warning_text_l2}\n")

        summary: str = str(review_data.get("overall_summary", "No overall summary provided."))
        markdown_parts.append(f"## AI-Generated Overall Summary\n\n{summary}\n")

        key_chars_any: Any = review_data.get("key_characteristics", [])
        key_chars: list[str] = (
            key_chars_any if isinstance(key_chars_any, list) and all(isinstance(i, str) for i in key_chars_any) else []
        )
        if key_chars:
            markdown_parts.append("## Key Architectural Characteristics (AI-Observed)\n")
            for char_item in key_chars:
                markdown_parts.append(f"- {char_item}")
            markdown_parts.append("\n")

        areas_disc_any: Any = review_data.get("areas_for_discussion", [])
        areas_disc: list[str] = (
            areas_disc_any
            if isinstance(areas_disc_any, list) and all(isinstance(i, str) for i in areas_disc_any)
            else []
        )
        if areas_disc:
            markdown_parts.append("## Potential Areas for Discussion (AI-Suggested)\n")
            for area_item in areas_disc:
                markdown_parts.append(f"- {area_item}")
            markdown_parts.append("\n")

        obs_patterns_any: Any = review_data.get("observed_patterns", [])
        obs_patterns: list[str] = (
            obs_patterns_any
            if isinstance(obs_patterns_any, list) and all(isinstance(i, str) for i in obs_patterns_any)
            else []
        )
        if obs_patterns:
            markdown_parts.append("## Observed Patterns & Structural Notes (AI-Identified)\n")
            for pattern_item in obs_patterns:
                markdown_parts.append(f"- {pattern_item}")
            markdown_parts.append("\n")

        coding_obs_any: Any = review_data.get("coding_practice_observations", [])
        coding_obs: list[str] = (
            coding_obs_any
            if isinstance(coding_obs_any, list) and all(isinstance(i, str) for i in coding_obs_any)
            else []
        )
        if coding_obs:
            markdown_parts.append("## Coding Practice Observations (AI-Noted)\n")
            for obs_item in coding_obs:
                markdown_parts.append(f"- {obs_item}")
            markdown_parts.append("\n")

        return "\n".join(markdown_parts)

    def pre_execution(self, shared_context: SLSharedContext) -> ProjectReviewPreparedInputs:
        """Prepare necessary data and context for generating the project review.

        Args:
            shared_context: The shared context dictionary.

        Returns:
            A dictionary containing data for the `execution` method, or indicating skip.
        """
        self._log_info("Preparing for project review generation.")
        try:
            config_any: Any = self._get_required_shared(shared_context, "config")
            config: ConfigDictInternal = config_any if isinstance(config_any, dict) else {}
            output_config_any: Any = config.get("output", {})
            output_config: OutputConfigDictInternal = output_config_any if isinstance(output_config_any, dict) else {}

            include_review_any: Any = output_config.get("include_project_review")
            include_review: bool = include_review_any if isinstance(include_review_any, bool) else False

            if not include_review:
                self._log_info("Project review generation is disabled in configuration. Skipping.")
                return {"skip": True, "reason": "Disabled in configuration"}

            abstractions_data: AbstractionsListInternal = self._get_required_shared(shared_context, "abstractions")  # type: ignore[assignment]
            if not isinstance(abstractions_data, list):
                raise TypeError("Abstractions data is not a list.")

            relationships_data: RelationshipsDictInternal = self._get_required_shared(shared_context, "relationships")  # type: ignore[assignment]
            if not isinstance(relationships_data, dict):
                raise TypeError("Relationships data is not a dict.")

            files_data: FileDataListInternal = self._get_required_shared(shared_context, "files")  # type: ignore[assignment]
            if not isinstance(files_data, list):
                raise TypeError("Files data is not a list.")

            prepared_inputs: ProjectReviewPreparedInputs = {
                "skip": False,
                "project_name": str(self._get_required_shared(shared_context, "project_name")),
                "abstractions_data": abstractions_data,
                "relationships_data": relationships_data,
                "files_data": files_data,
                "language": str(shared_context.get("language", "unknown")),
                "llm_config": self._get_required_shared(shared_context, "llm_config"),
                "cache_config": self._get_required_shared(shared_context, "cache_config"),
            }
            return prepared_inputs
        except (ValueError, TypeError) as e_prep_val:
            self._log_error("Preparation for project review failed due to missing/invalid data: %s", e_prep_val)
            return {"skip": True, "reason": f"Data preparation error: {e_prep_val!s}"}

    def execution(self, prepared_inputs: ProjectReviewPreparedInputs) -> ProjectReviewExecutionResult:
        """Generate the project review using an LLM.

        If an LlmApiError occurs, it is re-raised to be handled by the Node's retry logic.
        Other errors during validation or processing result in a fallback Markdown message.

        Args:
            prepared_inputs: The dictionary returned by the `pre_execution` method.

        Returns:
            A string containing the Markdown content for the project review,
            or None if skipped, or an error message string on failure.
        """
        if prepared_inputs.get("skip", True):
            reason_any: Any = prepared_inputs.get("reason", "N/A")
            self._log_info("Skipping project review execution. Reason: %s", str(reason_any))
            return None

        project_name: str = prepared_inputs["project_name"]
        self._log_info("Generating project review for '%s' using LLM...", project_name)

        abstractions_data: AbstractionsListInternal = prepared_inputs["abstractions_data"]  # type: ignore[assignment]
        relationships_data: RelationshipsDictInternal = prepared_inputs["relationships_data"]  # type: ignore[assignment]
        files_data: FileDataListInternal = prepared_inputs["files_data"]  # type: ignore[assignment]
        language: str = prepared_inputs["language"]
        llm_config: LlmConfigDictInternal = prepared_inputs["llm_config"]  # type: ignore[assignment]
        cache_config: CacheConfigDictInternal = prepared_inputs["cache_config"]  # type: ignore[assignment]

        prompt = ProjectReviewPrompts.format_project_review_prompt(
            project_name=project_name,
            abstractions_data=abstractions_data,
            relationships_data=relationships_data,
            files_data=files_data,
            language=language,
        )

        try:
            response_text = call_llm(prompt, llm_config, cache_config)  # This can raise LlmApiError
            validated_yaml_data = validate_yaml_dict(response_text, PROJECT_REVIEW_SCHEMA)
            markdown_content = self._format_review_yaml_to_markdown(validated_yaml_data, project_name)
            self._log_info("Successfully generated project review content.")
            return markdown_content
        except LlmApiError:  # Re-raise LlmApiError to trigger Node's retry/fallback
            self._log_error(
                "LLM call failed during project review generation. This error will be re-raised for retry/fallback."
            )
            raise
        except ValidationFailure as e_val:
            self._log_error("YAML validation failed for project review: %s", e_val)
            return f"# Project Review: {project_name}\n\n> AI-generated review validation failed: {e_val!s}"
        except (ValueError, TypeError, AttributeError, KeyError) as e_proc:  # Other processing errors
            self._log_error("Unexpected error processing project review: %s", e_proc, exc_info=True)
            return f"# Project Review: {project_name}\n\n> Unexpected error generating review: {e_proc!s}"

    def execution_fallback(  # Override from Node
        self, prepared_inputs: ProjectReviewPreparedInputs, exc: Exception
    ) -> ProjectReviewExecutionResult:
        """Handle fallback if all execution attempts for project review fail.

        Args:
            prepared_inputs: The data from the `pre_execution` phase.
            exc: The exception that occurred during the last execution attempt.

        Returns:
            A Markdown string indicating the failure.
        """
        project_name: str = prepared_inputs.get("project_name", "Unknown Project")
        self._log_error(
            "All attempts to generate project review for '%s' failed. Last error: %s", project_name, exc, exc_info=True
        )
        return f"# Project Review: {project_name}\n\n> AI-generated review could not be created after multiple attempts. Error: {exc!s}"  # noqa: E501

    def post_execution(
        self,
        shared_context: SLSharedContext,
        prepared_inputs: ProjectReviewPreparedInputs,
        execution_outputs: ProjectReviewExecutionResult,
    ) -> None:
        """Store the generated project review content in shared context.

        Args:
            shared_context: The shared context dictionary to update.
            prepared_inputs: Result from the `pre_execution` phase.
            execution_outputs: Markdown content of the project review, or None/error string.
        """
        if prepared_inputs.get("skip", True):
            shared_context["project_review_content"] = None
            self._log_info("Project review was skipped, 'project_review_content' set to None.")
            return

        shared_context["project_review_content"] = execution_outputs
        if execution_outputs and execution_outputs.strip():
            snippet = execution_outputs[:MAX_REVIEW_SNIPPET_LEN].replace("\n", " ")
            if len(execution_outputs) > MAX_REVIEW_SNIPPET_LEN:
                snippet += "..."
            self._log_info("Stored project review content in shared context (snippet: '%s').", snippet)
        else:
            self._log_warning("Project review content from execution was None or empty. Stored None.")


# End of src/sourcelens/nodes/n09_generate_project_review.py
